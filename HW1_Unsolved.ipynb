{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pourzand/CSCI499/blob/main/HW1_Unsolved.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overall guidelines:\n",
        "- Only change the cells which say:\n",
        "```# INSERT CODE/TEXT HERE```\n",
        "- Except Q1, other content will be covered in class starting Jan 17.\n"
      ],
      "metadata": {
        "id": "y86uNLRCqJ4H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q1. Probability (1)"
      ],
      "metadata": {
        "id": "29sBMdraQzcn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Say we have three random variables $A$ and $B$ and $C$. Note that we’re using standard probability\n",
        "theory notation where $P(A, B) = P(B, A)$, which simply means the joint probability of both A\n",
        "and B occurring.\n",
        "\n",
        "### Q1.1\n",
        "(0.5 point)\n",
        "\n",
        "Which of the following statements is always true?\n",
        "1. $P(A|B) = P(B|A)$\n",
        "2. $P(A, B) = P(A) + P(B)$\n",
        "3. $P(A, B, C) = P(A)P(B)P(C)$\n",
        "4. $P(A|B) = max(P(A),P(B))$\n",
        "5. $P(A, B, C) = P(A)P(C)$\n",
        "6. $P(A, B) = P(B|A)P(A)$\n",
        "7. $P(A, B, C) = P(A)P(B|A)P(C|A, B)$\n",
        "8. $P(A)=\\sum_{b ∈ \\text{domain}(B)} P(A, B=b)$\n",
        "9. $P(A)=\\sum_{b ∈ \\text{domain}(B)} P(A|B=b)P(B=b)$\n",
        "10.$\\log(1/P(A)) = 1 - log P(A)$"
      ],
      "metadata": {
        "id": "YBqKC9_qzHDy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q1.2\n",
        "(0.5 point)\n",
        "\n",
        "Now assume that A, B, and C are all independent of each other. Which of the above statements\n",
        "are now true?"
      ],
      "metadata": {
        "id": "gD6si1z2Izd6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**\\# INSERT TEXT HERE**\n",
        "\n",
        "Answer the two subparts here by listing which statement (just the statement numbers) are true."
      ],
      "metadata": {
        "id": "BUxcuDifubco"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q2. N-gram language model (3)\n",
        "\n",
        "Create a 5-gram language model trained on the Shakespeare dataset. Store the relevant probabilities P(word|context) in python dictionaries. Do not use any smoothing or back-off (until Question 4). Pay special attention to beginning and end of sequences in the modeling process."
      ],
      "metadata": {
        "id": "Qr835oX2Q4d4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import collections\n",
        "import random\n",
        "import sys\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "import math"
      ],
      "metadata": {
        "id": "VH7dZGzQQ7X1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "response = requests.get(url)\n",
        "response.raise_for_status() # Raise an exception for invalid HTTP status codes\n",
        "text_data = response.text\n",
        "len(text_data), text_data[:100]\n"
      ],
      "metadata": {
        "id": "TvlToTkkQ_mx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b8a23c6-90e6-4236-e213-ce25bd7dbfa4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1115394,\n",
              " 'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sample\n",
        "pos = random.randint(0, len(text_data) - 1000)\n",
        "print(text_data[pos:pos+100])"
      ],
      "metadata": {
        "id": "qrhtxdpuW3OT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fbe43f0-bf40-4dbf-897f-dea4b6c5ef03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " the beetle brows shall blush for me.\n",
            "\n",
            "BENVOLIO:\n",
            "Come, knock and enter; and no sooner in,\n",
            "But every \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocessing - do not change\n",
        "text_data = response.text\n",
        "text_data = text_data.replace(',',' , ').replace('.',' . ').replace('?',' ? ').replace('!',' ! ')\n",
        "text_data = text_data.replace('  ', ' ')\n",
        "text_data = text_data.replace('\\n\\n','\\n').replace('\\n',' </s> <s> ')\n",
        "text_data = '<s> ' + text_data + ' </s>'\n",
        "len(text_data), text_data[:100]"
      ],
      "metadata": {
        "id": "2WXecoY-jsHl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e54dfb29-8b41-4a10-aec0-cceca3187e4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1451204,\n",
              " '<s> First Citizen: </s> <s> Before we proceed any further , hear me speak .  </s> <s> All: </s> <s> ')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = text_data[-10_000:]\n",
        "text_data = text_data[:-10_000]\n",
        "len(text_data), len(test_data)"
      ],
      "metadata": {
        "id": "88hK3UwGhLPt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a446c672-8e1d-4509-b530-5dcf153f4967"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1431204, 10000)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# split words\n",
        "data = text_data.split(' ')\n",
        "len(data), data[:10]"
      ],
      "metadata": {
        "id": "RZXQmnhksMm1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a177aba7-a243-4652-81ea-92da7132ad8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(311889,\n",
              " ['<s>',\n",
              "  'First',\n",
              "  'Citizen:',\n",
              "  '</s>',\n",
              "  '<s>',\n",
              "  'Before',\n",
              "  'we',\n",
              "  'proceed',\n",
              "  'any',\n",
              "  'further'])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# UPDATE: Removing unseen tokens from test_data\n",
        "vocab = set(data)\n",
        "test_data = ' '.join([_ for _ in test_data.split(' ') if _ in vocab])\n",
        "print(test_data[:100])"
      ],
      "metadata": {
        "id": "DpsE6TO2QJXe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8624e683-c89a-405f-8af9-ad82e799fd3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "or dost unwillingly </s> <s> What I command , I'll rack thee with old cramps ,  </s> <s> Fill all th\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "five = collections.defaultdict(lambda:0)\n",
        "four = collections.defaultdict(lambda:0)\n",
        "tri = collections.defaultdict(lambda:0)\n",
        "bi = collections.defaultdict(lambda:0)\n",
        "uni = collections.defaultdict(lambda:0)\n",
        "\n",
        "# INSERT CODE HERE\n"
      ],
      "metadata": {
        "id": "L8iunWOah5QR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation - Do not change\n",
        "assert five[('Before','we','proceed','any','further')] == 1.0 # prob of last given prev 4\n",
        "assert four[('<s>','First','Citizen:','</s>')] == 1.0 # prob of last given prev 3\n",
        "assert tri[('<s>','First','Citizen:')] == 0.172 # prob of last given prev 2\n",
        "assert round(bi[('First','Citizen:')],3) == 0.169 # prob of last given prev 1\n",
        "assert round(uni[('Citizen:')],5) == 0.00031 # prob of last"
      ],
      "metadata": {
        "id": "j54tdjyD-3f9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the dictionaries containing unigram, bigram, ... five-gram probabilities, return probability P(word|context) as a single scalar. Here context is a list of previous tokens, e.g., [`You`, `may`, `do`, `as`, `you`] and word is a single potential next word, e.g., `like`."
      ],
      "metadata": {
        "id": "gOuU211BCR2T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_prob_from_lm(context=[], word=\"\"):\n",
        "  return 0\n",
        "  # INSERT CODE HERE\n"
      ],
      "metadata": {
        "id": "TkThhB-G1Pnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_prob_from_lm([\"<s>\", \"First\"], \"Citizen:\")"
      ],
      "metadata": {
        "id": "5L1wU1LnKpRC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q3. Evaluate Perplexity (2.5)\n",
        "\n",
        "Now let's evaluate the models quantitively using the intrinsic metric **perplexity**.\n",
        "\n",
        "Recall perplexity is the inverse probability of the test text\n",
        "$$\\text{ppl}(w_1, \\dots, w_n) = p(w_1, \\dots, w_n)^{-\\frac{1}{N}}$$\n",
        "\n",
        "For an n-gram model, perplexity is computed by\n",
        "$$\\text{ppl}(w_1, \\dots, w_n) = (\\prod_i p(w_{i+n}|w_i^{i+n-1})^{-\\frac{1}{N}}$$\n",
        "\n",
        "To get rid of numerical issue, we usually compute through:\n",
        "$$\\text{ppl}(w_1, \\dots, w_n) = \\exp(-\\frac{1}{N}\\sum_i \\log p(w_{i+n}|w_i^{i+n-1}))$$\n",
        "\n",
        "\n",
        "*Input:*\n",
        "\n",
        "+ **prob_function**: the `get_prob_from_lm` you implemented in Q2.\n",
        "+ **data**: test data is a string of text containing multiple words.\n",
        "\n",
        "*Output:*\n",
        "+ the perplexity of given data under a language model defined by **prob_function**\n",
        "\n",
        "Note that you do NOT need to optimize the language model in any way so as to minimize perplexity. Your reported perplexity will have no correlation with your score on this assignment, as far as it is implemented correctly."
      ],
      "metadata": {
        "id": "RNG3-Fx5_-DJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_perplexity(prob_function=get_prob_from_lm, data=test_data):\n",
        "  ...\n",
        "  # Hint: something = prob_function(context, word)\n",
        "  # INSERT CODE HERE"
      ],
      "metadata": {
        "id": "ZmH82w2LDA1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simple_data = 'Before we proceed any further , hear me speak .'\n",
        "compute_perplexity(data=simple_data)"
      ],
      "metadata": {
        "id": "VjXSmCU1DXCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can the perplexity of a sentence under any 5-gram model be 1? Explain.\n",
        "\n",
        "**\\# INSERT TEXT HERE**\n"
      ],
      "metadata": {
        "id": "ZsfSFUf-u3BT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "compute_perplexity(data=test_data)"
      ],
      "metadata": {
        "id": "fuZb0Zo5Ddft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explain** the resulting perplexity above on test data.\n",
        "Conjecture (in text, not code) how it could be improved?\n",
        "\n",
        "**\\# INSERT TEXT HERE**"
      ],
      "metadata": {
        "id": "1_X3zkY3V4_d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q4. Interpolation Smoothing (1)\n",
        "\n",
        "Using the dictionaries containing unigram, bigram, ... five-gram probabilities, return probability P(word|context) where context is the previous string, e.g., `You may do as you` and word is a single potential next word, e.g., `like`.\n",
        "\n",
        "Unlike in Q2, this time use interpolation smoothing as discussed in class."
      ],
      "metadata": {
        "id": "wUlraNm_jKZk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "l1=0.2; l2=0.2; l3=0.2; l4=0.2; l5=0.2\n",
        "# lambdas needed for Interpolation - you do NOT need to change these for the assignment\n",
        "def get_prob_from_lm_with_interpolation(context=[], word=\"\"):\n",
        "  # INSERT CODE HERE\n",
        "get_prob_from_lm_with_interpolation(['Before', 'we', 'proceed', 'any'],\n",
        "                                    'further')"
      ],
      "metadata": {
        "id": "QypKY49tzwMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we re-evaluate perplexity with the new probability estimates (interpolated)."
      ],
      "metadata": {
        "id": "V3VKjyhyEVcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "compute_perplexity(prob_function=get_prob_from_lm_with_interpolation,\n",
        "                   data=simple_data)"
      ],
      "metadata": {
        "id": "h86ON9wtEQ6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compute_perplexity(prob_function=get_prob_from_lm_with_interpolation,\n",
        "                   data=test_data)"
      ],
      "metadata": {
        "id": "VYLLgaYgERTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q5. Text Generation (2.5)\n",
        "\n",
        "Our final goal is to generate/infer/sample from the language model that we have created.\n",
        "Given a prefix or prompt, the model can generate text according to estimated next word probabilities (similar to any LLM, e.g., GPT).\n",
        "\n",
        "Complete the following function using the dictionaries already available to sample (you may use `random.choice` function) from the distribution of possible next tokens. Remember to randomly sample (from a weighted distribution over all possible next words in the vocabulary) instead of picking only the most likely next word.\n",
        "\n",
        "Input:\n",
        "- dictionaries `uni`, `bi`, `tri`, `four`, `five`\n",
        "- **n_generate**: number of maximum words to infer. E.g., `2`\n",
        "- **prefix**: the context on which to condition text generation. E.g., `as you`\n",
        "\n",
        "Output: string output containing prefix and generated words separated by whitespace. E.g., `as you like it`"
      ],
      "metadata": {
        "id": "UscwpcjrtHf5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prefix = 'as you say you' # \"hear me\"\n",
        "def generate(prob_fn=get_prob_from_lm_with_interpolation,\n",
        "             prefix=prefix, n_generate=20):\n",
        "  # INSERT CODE HERE"
      ],
      "metadata": {
        "id": "FUtpND2XtH11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate(prob_fn=get_prob_from_lm)"
      ],
      "metadata": {
        "id": "MJJweT7Gxgkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate(prob_fn=get_prob_from_lm_with_interpolation)"
      ],
      "metadata": {
        "id": "bdEGwdhN0t_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare the outputs of the two language models. Why do you think a difference exists?\n",
        "\n",
        "**\\# INSERT TEXT HERE**"
      ],
      "metadata": {
        "id": "gZzRMpNBxjKN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The End\n",
        "Voila! If you were able to complete the assignment, you have successfully created a language model in barebones Python! You have trained it on a text corpus and can use it to generate arbitrary text just like GPT-3!"
      ],
      "metadata": {
        "id": "yTw020PNa5_Z"
      }
    }
  ]
}